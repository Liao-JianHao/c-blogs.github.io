---
layout:     post
title:      爬虫(三)
subtitle:   scrapy框架、分布式爬虫
date:       2019-07-26
author:     Mr.C
header-img: img/scrapy.jpg
catalog: true
tags:
    - scrapy
    - scrapy_redis
---

## scrapy框架

> scrapy使用了Twisted异步网络框架，它是一个用于爬取网络数据、提取结构性数据的框架

![scrapy流程](http://www.c-blogs.cn/img/scrapy流程.png)

> scrapy的三个对象：request、response、item
>> 各模块功能： <br> 
引擎：数据和信号的传递 <br> 
调度器：任务队列 <br> 
下载器 <br> 
爬虫：起始url和解析 <br> 
管道：保存数据 <br> 
中间件：定制化操作


- `scrapy startproject myspider`：创建项目

    ![爬虫框架目录](http://www.c-blogs.cn/img/爬虫框架目录.png)
    
- `scrapy genspider <爬虫名字> <允许爬取的域名>`：创建爬虫
- `scrapy crawl <爬虫名字>`：运行scrapy
    - `--nolog`：关闭日志

#### 技术点

> spider通过方法进行数据提取等操作
>> 在**spider文件**中**parse()函数**中使用yield返回数据

- response 对象
    - response 对象常用的属性
        - `url`：当前响应的url地址
        - `request.url`：当前响应对应的请求的url地址
        - `headers`：响应头
        - `requests.headers`：当前响应头的请求体 `boby`：响应体，也就是html代码，byte类型
        - `status`：响应状态码
    - response 对象常用方法
        - `xpath()`：通过xpath定位对应元素
        - `extract()`：提取数据
        - `extract_first()`：提取第一条数据

- Reqeust对象
    
    > 通常不使用Request发送post请求，而是用`FormRequest`，通过`formdata`来传递数据

    - `scrapy.Reqeust(url[,callback,method='GET',headers,body,cookies,meta,dont_filter=False])`
        - []  方括号中是可选参数
        - `callback`：表示当前url的响应交给哪个函数处理
        - `meta`：实现数据在不同解析函数中传递，meta默认带部分数据：下载延迟、请求深度
        - `dont_filter`：默认False，会过滤请求url，即请求过的url地址不会继续请求；是否对url队列做去重
        - `method`：请求方法
        - `headers`：接受字典，其中不包括cookies
        - `cookies`：接受字典，专门放cookies
        - `body`：接受json字符串，为POST的数据

> 在**pipeline.py**利用**pipeline管道**来处理(保存数据)
>> 必须要有`process_item`方法和接受`item`、`spider`参数，最后需要`return item`；管道,值小的管道先运行,值最好不要大于1000

- 通过json模块的`ensure_ascii`：ensure_ascii参数在写入数据时以中文保存

- 通过`open_spider`和`close_spider`代替`__init__`和`__del__`方法，其中可以通过方法中的`spider`参数筛选，进行选择性的使用爬虫

> 在**items.py**中进行**数据建模** <br> 定义item即提前规划所需抓的字段，防止手误；配合注释更清晰采集的数据；scrapy组件需要item做支持，比如imagesPipeline管道类

#### crawlspider

> crawlspider就可以匹配满足条件的url地址，组装成Request对象后自动发送给引擎，同时能够指定callback函数；也就是说crawlspider爬虫可以按照规则自动获取链接
>> crawlspider中不能再有parse为名的数据提取方法

- `crawl genspider -t <爬虫名字> <域名>`：创建crawlspider爬虫

- `Rule()`：链接处理规则对象
    - `LinkExtractor`类：链接提取器
        - `allow`：接受正则匹配规则
        - `deny`：满足正则的不会被提取，优先级高于allow
        - `allow_domains`：会被提取的链接的domains(url范围)
        - `deny_domains`：不会提取链接的domains(url范围)
        - `restrict_xpath`：使用xpath规则进行匹配
    - `callback`：回调函数
    - `follow`：决定后面是否继续用链接提取器
    - `process_links`：当链接提取器LinkExtractor获取到链接列表的时候调用该参数的方法，这个方法可以用来过滤url，此方法执行后才执行callback指定的方法

#### scrapy的中间件

> 分为：下载中间件、爬虫中间件
>> 作用：预处理request和response对象 <br> 
在默认情况下scrapy默认的情况下，两种中间件都在`middlewares.py`一个文件中 <br> 
爬虫中间件使用方法和下载中间件相同，且功能重复，通常使用下载中间件

- 使用scrapy的中间件随机UA的方法

- 使用scrapy的中间件代理ip的方法
    
    > 如果代理需要进行账户密码验证，需要转换为base64编码；request.headers["Proxy-Authorization"] = proxyAuth
    >> `'Basic ' + base64(密码)`：注意 Basic有个空格；`reqest.meta["proxy"] = proxy`

- 使用scrapy的中间件与selenium配合使用

#### scrapy_splash组件

> scrapy_splash能够模拟浏览器加载js，并返回js运行后的数据
>> splash类似selenium，能够像浏览器一样访问请求对象中的url地址 <br> 
能够按照该url对应的响应内容依次发送请求 <br> 
并将多次请求对应的多次响应内容进行渲染 <br> 
最终返回后的response响应对象

- `sudo docker scrapinghub/splash`：通过docker拉取splash镜像
    - `sudo docker run -p 8050:8050 scrapinghuub/splash`：前台运行
    - `sudo docker run -d -p 8050:8050 scrapinghuub/splash`：后台运行

- `pip install scrapy-splash`：python安装scrapy_splash包

~~~python
"""scrapy_splash配置项"""


# 渲染服务的url
SPLASH_URL = 'http://127.0.0.1:8050'

# 下载器中间件
DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}

# 去重过滤器
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'

# 使⽤Splash的Http缓存
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False
~~~

- 重写 `def start_request(self):` 函数

~~~python
def start_request(self):
    yield SplashRequest(
        url = self.start_urls[0],
        callback=self.parse,
        args={'wait':10},  # 最大超时时间，单位：秒
        endpoint='render.html'
    )
~~~

## scrapy_redis

![scrapy_redis](http://www.c-blogs.cn/img/scrapy_redis.png)

![分布式scripy_redis](http://www.c-blogs.cn/img/分布式scripy_redis.png)


> scrapy_redis中，所有待抓取的对象和去重的指纹都存在公用的redis中；执行爬虫后，会发现redis中多了三个键：**requests**(Scheduler队列，存放待请求的request对象，获取的过程是pop操作，即获取一个移除一个)、**dupefilter**(指纹集合，存放的是已经进入Scheduler队列中的request对象的指纹，指纹默认由请求方式、url、请求体组成)、**items**(存放获得到的item信息，在pipeline中开启RedisPipeline才会存入)
>> `git clone https://github.com/rolando/scrapy-redis.git`

- 通过持久请求队列和请求的指纹集合来实现：
    - 断点续爬
    - 分布式爬虫
        - 编写普通爬虫
            - 创建项目
            - 明确目标
            - 创建爬虫
            - 保存内容
        - 改造成分布式爬虫
            - 改造爬虫：
                - 导入scrapy_redis中的分布式爬虫类、
                - 继承类
                - 注销 `start_url`和`allowed-domains`
                - 设置`redis_key`获取`start_urls`
                - 设置`__init__`获取允许的域
        - 改造配置文件
            - copy配置参数

~~~python
def __init__(self, *args, **kwargs):
    domain = kwargs.pop('domain', '')
    self.allowed_domain = list(filter(None, domain.split(',')))
    super(currentRedisSpiderClass, self).__init__(*args, **kwargs)
~~~

~~~python
"""配置文件"""

# Scrapy settings for example project
#
# For simplicity, this file contains only the most important settings by
# default. All the other settings are documented here:
#
#     http://doc.scrapy.org/topics/settings.html
#
SPIDER_MODULES = ['xxx.spiders']
NEWSPIDER_MODULE = 'xxx.spiders'

USER_AGENT = 'scrapy-redis (+https://github.com/rolando/scrapy-redis)'

# 设置重复过滤器的模块
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"

# 设置调度器，scrapy_redis.Scheduler.RFPDupeFilter
SCHEDULER = "scrapy_redis.scheduler.Scheduler"

# 设置当爬虫结束的时候是否保持redis数据库中的去重集合与任务队列
SCHEDULER_PERSIST = True

#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue"
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderQueue"
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderStack"

ITEM_PIPELINES = {
    # 当开启该管道，该管道会将数据存到redis数据库中
    'xxx.pipelines.ExamplePipeline': 300,
    'scrapy_redis.pipelines.RedisPipeline': 400,
}

# 设置reids数据库
REDIS_URL = 'redis//127.0.0.1'

LOG_LEVEL = 'DEBUG'

# Introduce an artifical delay to make use of parallelism. to speed up the
# crawl.
# 下载延迟
DOWNLOAD_DELAY = 1
~~~

#### Gerapy 分布式爬虫管理框架

`pip install gerapy`：python下载gerapy包
`gerapy init`：创建一个项目
`gerapy migrate`：对数据库进行初始化，会生成一个SQLite数据库，保存主机配置和部署版本信息等
`gerapy runserver`：启动gerapy


## scrapyd部署scrapy项目

`pip install scrapyd`：scrapyd服务
`pip install scrapyd-client`：scrapyd客户端
`scrapyd-deploy 部署名 -p 项目名`：部署项目到scrapyd
`curl http://localhost:6800/scheduler.json -d project=project_name -d spider=spider_name`：启动项目
`curl http://localhost:6800/cancel.json -d project=project_name -d job=jobid`：关闭爬虫

> 通过JSON API来部署爬虫项目和控制爬虫运行，scrapyd是一个守护进程，监听爬虫和运行和请求
>> json api 本质就是post请求的webapi

~~~python
"""部署项目的scrapy.cfg文件"""

[settings]
default=项目名.setting  # 项目的配置文件

[deploy:部署名(部署名可以自定义)]
url = http://localhost:6800/
project = 项目名(创建爬虫项目时用的名称)
~~~
