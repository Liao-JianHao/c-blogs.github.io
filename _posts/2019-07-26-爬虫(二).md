---
layout:     post
title:      爬虫(二)
subtitle:   selenium、反爬虫
date:       2019-07-26
author:     Mr.C
header-img: img/爬虫wallpaper2.jpg
catalog: true
tags:
    - selenium
    - requests
    - xpath


---


## Selenium框架

> selenium是自动化测试框架，selenium能够大幅度降低爬虫的编写难度，但也会大幅度降低爬虫爬取速度 <br> 
webdriver本质是一个web-server，对外提供webapi，其中封装了浏览器的各种功能
>> 通常在开发过程中需要查看运行过程中的各种情况所有使用有**头浏览器** <br> 
在项目完成进行部署的时候，服务器版的操作系统必须使用**无头浏览器**才能正常运行

- 前提
    1. 安装浏览器对应的driver
    2. 将浏览器driver放入环境中

- selenium开启无界面模式
    
    ~~~python
    options = webdriver.ChromeOptions()  # 实例化配置对象
    options.add_argument("--headless")  # 配置对象添加开启无界面模式的命令
    options.add_argument("--disable-gpu")  # 配置对象禁用gpu的命令
    options.add_argument("--proxy-server=http//xxx.xxx.xxx.xxx:xxxx")  # 配置对象添加使用代理ip的命令；更换ip代理，必须重启浏览器
    options.add_argument("--user-agent=浏览器版本")  # 更换浏览器版本
    driver = webdriver.Chrome(chrome_options=options)
    ~~~
    

- driver对象常用属性和方法
    - `driver.page_source` 当前标签页浏览器渲染后的网页源代码
    - `driver.current_url`  当前标签的url
    - `driver.close()`  关闭当前标签页，如果只有一个标签则关闭整个浏览器
    - `driver.quit()`  关闭浏览器
    - `driver.forward()`  页面前进
    - `driver.back()`  页面后退
    - `driver.screen_shot(img_name.png)`  页面截图

~~~python
from selenium import webdriver


url = 'https://www.baidu.com'

driver = webdriver.Chrome(executable_path='./chrmoedriver')  # 创建浏览器对象，如果在系统中已设置环境变量则不填
driver.get(url)
driver.find_element_by_id('kw').send_keys('python')  # 定位id属性值中的标签素；输入搜索的关键字
driver.find_element_by_id('su').click()  # 定位id属性值中的标签；点击百度搜索
driver.quit()  # 推出浏览器
~~~

- driver对象定位标签元素获取标签对象的方法

    > find_element 和 find_element(s) 的区别：多来个s就会返回列表，find_element匹配不到就抛出异常，find_element(s)匹配不到就会返回空列表

    - `driver.find_element_by_id()`  返回一个元素
    - `driver.find_element(s)_by_class_name()`  根据类名获取元素列表
    - `driver.find_element(s)_by_name()`  根据标签的name属性值返回包含标签对象的列表
    - `driver.find_element(s)_by_xpath()`  返回一个包含元素的列表
    - `driver.find_element(s)_by_link_text()`  根据连接文本获取元素列表
    - `driver.find_element(s)_by_partial_link_text()`  根据链接包含的文本获取元素列表
    - `driver.find_element(s)_by_tag_name()`  根据标签名获取元素列表
    - `find_element(s)_by_css`  根据css选择器来获取元素列表
    
- 标签对象提取文本内容和属性值
    - `element.text`  通过定位获取标签对象的text属性，获取文本内容
    - `element.get_attribute('属性名')`  通过定位获取标签对象传入的属性名，获取属性的值

- selenium标签页的切换
    - `driver.window_handles`  获取当前页面所有标签的句柄构成列表；窗口句柄是值标签页对象的标识
    - `driver.swich_to.window()`  根据标签页句柄列表索引下标进行切换
    - `driver.swich_to.frame(frame_element)`  selenium默认是访问不了iframe中的内容，通过该技术解决

- selenium对cookie的处理
    - `driver.get_cookies()`  获取当前标签页的全部cookie
    - - `driver.delete_all_cookies()`  删除所有cookie
    - - `driver.delete_cookie('CookieName')`  删除一条cookie

- selenium控制js代码
    - `driver.execute_script(js)`  执行js代码
    
    ~~~python
    js = 'window.scrollTo(x, y)'  # x是水平移动，y是垂直移动;浏览器向下滚动
    driver.execute_script(js)
    ~~~


#### 使用selenium框架登录163邮箱案例

~~~python
from selenium import webdriver
import time


url='https://mail.163.com/'
loginname=''#163邮箱账号
password=''#密码

browser = webdriver.Chrome()
browser.get(url)
browser.maximize_window()#窗口最大化
input2 = browser.find_element_by_id("switchAccountLogin").click()  # 获取点击登录id元素，并点击
browser.switch_to.frame(0)  # 切换到登录框
browser.find_element_by_name('email').clear()  # 清除旧email值
browser.find_element_by_name('email').send_keys(loginname)  # 填写新email值
browser.find_element_by_name('password').clear()
browser.find_element_by_name('password').send_keys(password)
browser.find_element_by_id('dologin').click()  # 获取登录id元素，并点击登录
# browser.switch_to_default_content()  # 退出登录框
# browser.switch_to.frame('x-URS-iframe')  # 进入确认登录框
# browser.find_element_by_link_text("继续登录").click()
time.sleep(5)
browser.quit()
~~~

- 页面等待
    - 强制等待：`time.sleep()`，缺点：不智能，设置时间太短，元素没加载出来，设置时间太长，则浪费时间
    - 隐式等待：`driver.implicitly_wait(20)` 最长等待20秒，设置时间，加载完就继续执行，没加载完则报超时加载
    - 显示等待
    
    ```python
    """显示等待"""
    from selenium.webdriver.support.wait import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.common.by import By
    
    WebDriverWait(driver, 20, 0.5).until(EC.presence_of_element_located((By.LINK_TEXT, '元素')))
    
    # 参数20表示最长等待20秒，参数0.5表示0.5秒检查一次标签是否存在
    # EC.presence_of_element_located((By.LINK_TEXT, '元素'，表示通过链接文本内容定位标签

## 反爬手段和思路

> 爬虫占总PV比例较高，对服务器压力大 <br> 
资源被批量抓走，丧失竞争力 <br> 
告状爬虫成功率小
>> 反爬的三个方向: <br> 
**基于身份识别进行反爬** <br> 
**基于爬虫行为进行反爬** <br>
**基于数据加密进行反爬** <br>

#### 基于身份识别进行反爬

- 通过headers字段来反爬
    - 通过User-Agent来反爬
    - 通过referer字段等字段来反爬
    - 通过cookie来反爬
- 通过请求参数来反爬
    - 通过静态页面文件中获取请求数据
    - 通过发送请求获取请求数据
    - 通过js生成请求参数
    - 通过验证码来反爬

#### 基于爬虫行为进行反爬

- 基于请求频率或总请求量
    - 通过请求ip/账户单位时间总请求数量进行反爬
    - 通过同一ip/账户请求之间的间隔进行反爬
    - 通过对请求ip/账户每天请求次数设置阈值进行反爬

- 根据爬取行为进行反爬
    - 通过js实现跳转来反爬
    - 通过蜜罐(陷阱)获取爬虫ip(或者代理ip)，进行反爬
    - 通过假数据反爬
    - 阻塞任务队列
    - 阻塞网络IO
    - 运维平台综合审计

#### 基于数据加密进行反爬

- 对响应中含有的数据进行特殊化处理
    - 通过自定义字体来反爬
    - 通过css来反爬
    - 通过js动态生成数据进行反爬
    - 通过数据图片化反爬
    - 通过编码格式进行反爬
    
#### 图片识别引擎

- 安装(两步骤)
    - 引擎的安装：`sudo apt install tesseract-ocr`
    - 安装Python模块：`pip install pillow` 和 `pip install pytesseract`

- 运行方式
    - 命令：`tesseract xxx.jpg/png result`  可以直接在终端通过命令运行,result为保持文件名，保存格式txt
    - 代码
    
    ~~~python
    from PIL import Image
    import pytesseract
    
    
    image = Image.open('test.jpg')  # 打开一个图片
    result = pytesseract.image_to_string(image)  # 将图片转换为字符串
    print(result)
    ~~~

- 第三方API
    - [云打码](http://www.yundama.com/)
    - [极验打码](http://jiyandoc.c2567.com/)

#### js解析

- 定位js文件
    - 通过initiator定位js文件
    - 通过search 搜索关键字定位js文件
    - 通过Event Listeners定位js文件
    
#### js2py模块

> js2py是一个js翻译工具，也是一个通过纯python实现的js的解释器
    
~~~python
import js2py
import requests


context = js2py.EvalJs()  # 创建js执行环境

js = requests.get('js文件请求', headers=headers).content.decode()  # 获取js文件

context.execute(js)
~~~

#### python中的hash

~~~python
import hashlib

data = python

md5 = hashlib.md5()  # 创建hash对象

md5.update(data.encode())  # 向hash对象中添加需要做hash运算的字符串

result = md5.hexdigest()  # 获取字符串的hash值
print(result)
~~~


**注：原创文章，转载请注明出处**
