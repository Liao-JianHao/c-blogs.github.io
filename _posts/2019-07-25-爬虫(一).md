---
layout:     post
title:      爬虫(一)
subtitle:   requests、jsonpath、xpath
date:       2019-07-25
author:     Mr.C
header-img: img/爬虫wallpaper1.jpg
catalog: true
tags:
    - selenium
    - requests
    - xpath


---


## 爬虫的概念

> 模拟浏览器，发送请求，获取响应


```
graph LR
爬虫分类-->通用爬虫
通用爬虫-->特点:爬取网站数量没有上限
通用爬虫-->示例:搜索引擎
爬虫分类-->聚焦爬虫
聚焦爬虫-->特点:爬取网站数量有上限,有明确目标
聚焦爬虫-->分类
分类-->功能性爬虫
功能性爬虫-->特点:不获取数据,只为实现某一功能
功能性爬虫-->示例:投票/抢票
分类-->数据增量爬虫
数据增量爬虫-->特点:获取数据后用于后续分析
数据增量爬虫-->类别
类别-->url与数据同时变化
url与数据同时变化-->整条新数据
类别-->url不变,数据变化
url不变,数据变化-->数据部分更新
```

#### 浏览器和爬虫的请求过程

- 数据类型
    > 骨骼文件：html静态文件 <br> 
    肌肉文件：js/ajax请求 <br> 皮肤：css/font/图片

- 浏览器
    - 发送所有请求，进行渲染

- 爬虫
    - 只发送指定请求，不会渲染

> 抓包过程：根据发送请求流程分别在 骨骼/肌肉/皮肤 响应中查找数据

## http协议

#### HTTP(超文本传输协议)

- 超文本：是指超过文本，不仅限于文本；还包括图片、音频、视频等文件
- 传输协议：固定格式来传递转换成字符串的超文本内容(明文)

#### HTTPS(HTTP + SSL安全套接字层)

- SSL对传输的内容(超文本，也就是请求体和响应体)进行加密

#### 常见请求头

- Host：域名
- content-Type：请求内容的类型
- Connection：连接类型
- Upgrade-lnsecure-Requests(升级为HTTPS请求)
- User-Agent：用户代理，提供系统信息和浏览器信息
- Referer：页面跳转处、防盗链
- Authorization：HTTP协议中需要认证资源的认证信息
- Cookie：状态保持

#### 常见状态码

| 状态码|含义|
|------|-----|
|200|请求成功|
|302|重定向|
|400|无法处理请求|
|404|资源未找到|
|405|请求方式不支持|
|501|服务器不认识|
|503|服务器维护或负载过重未能应带|

> 503状态码：在响应中可能会携带Retry-After响应头；有可能是因为爬虫频繁访问url，使服务器忽视爬虫的请求，最终返回503响应状态码
>> 所有状态码都不可信，一切以是否从抓包得到的响应中获取到数据为准 <br> 
Network中中抓包得到的源码才是判断依据，elements中的源码是渲染后的源码不能作为判断标准

#### http协议的八种动作

- GET

    向特定的路径资源发出请求

- POST

    向指定路径资源提交数据进行处理请求(一般用于提交表单或上传文件)

- OPTIONS

    返回服务器针对特定资源所支持的HTTP请求方法
    
- HEAD

    向服务器索与GET请求相一致的响应，只不过响应体将不会被返回

- PUT

    从客户端向服务器传送的数据取代指定的文档的内容
    
- DELETE

    请求服务器删除指定的页面
    
#### HTTP/2

> 数据流 stream：已建立的连接内的双向字节流，可以承载一条或多条消息 <br> 
消息 Message：于逻辑请求或响应消息对应的完整的一系列帧 <br> 
帧 Frame：HTTP/2 通讯的最小单位，每个帧都包含帧头，至少也会标识出当前帧所属的数据流
>> HTTP/2 采用的是二进制方式进行传输

## [requests](http://2.python-requests.org/zh_CN/latest/index.html)模块

> 发送http请求，获取响应数据

> get请求：需要url参数 <br> 
post请求：需要url、data参数；data为请求体的字典
>> post数据来源： <br> 
1.固定值    抓包比较不变值
2.输入值    抓包比较根据自身变化值 <br> 
3.预设值(静态文件中)    需要提前从静态html中获取
4.预设值(发请求)    需要对指定地址发送请求获取数据
5.在客户端生成的     分析js，模拟生成数据

- response对象

> response.url：响应的url <br> response.status_code：响应的状态码 <br> 
response.request.headers：响应对应的请求体 <br> 
response.headers：响应头 <br> 
response.request._cookies：响应对应的请求cookies，返回cookieJar类型 <br> 
response.cookies：响应的cookie <br> 
response.json()：自动将json字符串类型的响应内容转换为python对象 <br> 

~~~python
import requests


headers = {}  # 构建请求头,User-Agent和Cookie

data = {'wd': 'python'}  # 查询参数

cookie_list = xxx.split('; ')
cookies = {cookie.split('=')[0]: cookie.split('=')[-1] for cookie in cookie_list}  # 字典推导式处理cookie键值

url = "https://www.baidu.com/s?"  # 构建请求url接口
response = requests.get(url, headers=headers, params=data, cookies=cookies, timeout=3)  # 请求设置的参数；timeout设置超时请求

print(response.text)  # 获取源码的str类型数据
print(response.content)  # 获取源码的bytes类型数据，可以进行docode操作

# response.content.decode() 解决中文乱码
# 常见编码字符集：utf-8、gbk、gb2312、ascii
~~~

- cookieJar对象

> 使用request获取的response对象，具有cookies属性。该属性值是一个cookieJar类型，包含了对方服务器设置的本地cookie

~~~python
import requests

url = "http://www.baidu.com"

response = requests.get(url)

print(response.cookies)

dict_cookies = requests.utils.dict_from_cookiejar(response.cookies)
print(dict_cookies)  # 字典类型

jar_cookies = requests.utils.cookiejar_from_dict(dict.cookies)
print(jar_cookies)  # jar类型
~~~

- 代理ip
    - 透明代理(Transparent Proxy)：透明代理虽然可以直接“隐藏”ip地址，但是还是可以查到你是谁
    
    ~~~python
    REMOTE_ADDR = Proxy IP
    HTTP_VIA = Proxy IP
    HTTP_X_FORWARDED_FOR = Your IP
    ~~~

    - 匿名代理(Anonymous Proxy)：别人只能知道你用了代理，无法知道你是谁
    
    ~~~python
    REMOTE_ADDR = Proxy IP
    HTTP_VIA = Proxy IP
    HTTP_X_FORWARDED_FOR = proxy IP
    ~~~
    
    - 高匿代理(Elite Proxy)：让别人根本无法发现你是在用代理
    
    ~~~python
    REMOTE_ADDR = Proxy IP
    HTTP_VIA = not determined
    HTTP_X_FORWARDED_FOR = not determined
    ~~~
    
- 免费代理地址
    - [快代理](https://www.kuaidaili.com/free/)
    - [米扑代理](https://proxy.mimvp.com/free.php)

~~~python
import requests

proxies = {
    'http': 'http://xxx.xxx.xxx.xxx',
    'https': 'https://xxx.xxx.xxx.xxx',
}  # 根据网站所使用的协议不同，需要使用相应协议的代理服务，可分为三种：http代理、https代理、sooks隧道代理

url = "http://www.baidu.com"

response = requests.get(url, proxies=proxies)
~~~

- CA证书

> 保密性 - 只有收件人才能阅读信息 <br> 
认证性 - 确认信息发送者的身份 <br> 
完整性 - 信息在传递过程中不会被篡改 <br> 
不可抵赖性 - 发送者不能否认已发送的信息 <br> 
保证请求者与服务者的数据交换的安全性

~~~python

url = ''

response = reqeuests.get(url, verify=Flase)

print(response.content)
~~~

- session

> requests模块中的Session类 能够自动处理发送请求获取响应过程中生产的cookie，进而达到状态保持的目的

~~~python
import requests


url = ''

session = requests.session()
response = session.get(url, data )
~~~

## 数据提取

- 响应分类
    - 结构化
        - json数据：json模块、re模块、jsonpath模块等
        - xml数据：re模块、lxml模块等
        
        > xml是⼀种可扩展标记语⾔，样⼦和html很像，功能更专注于对传输和存储数据

    - 非结构化
        - html：re模块、lxml模块、beautifulsoup模块、pyquery模块等

#### jsonpath模块

> 使用于多层嵌套的复杂字典，根据key和下标来批量提取value

![jsonpath语法](http://www.c-blogs.cn/img/jsonpath语法.png)

~~~python
from jsonpath import jsonpath


data = {'key1': {'key2': {'key3': {'key4': 'python'}}}}

print(jsonpath(data, '$..key4')[0])  # 第一个参数为数据，第二个参数为jsonpath语法对数据的匹配

# 常用jsonpath语法：
# $：根节点
# .：子节点
# ..：子孙节点
~~~

#### lxml模块

> - [XPath](http://www.w3school.com.cn/xpath/index.asp)(xml path language)是一门在 HTML/XML 文档中查找信息的语言，对文档中的元素和属性进行遍历
> - lxml模块可以利用xpath规则语法，来快速获取文档特定元素及获取节点信息

- xpath基础语法

    ![xpath语法](http://www.c-blogs.cn/img/xpath语法.png)

- xpath 节点修饰语法

    ![xpath修饰语法](http://www.c-blogs.cn/img/xpath修饰语法.png)
    
    > 补充：contains()    包含

- xpath 通配符

    ![xpath通配符](http://www.c-blogs.cn/img/xpath通配符.png)
    
    > 补充：| 或
    
~~~python
from lxml import etree

html = etree.HTML(text)  # etree.HTML() 能够自动补全html缺失的标签
print(etree.tostring(html))  # 转换为bytes类型
print(html.xpath("xpath语法规则"))
~~~


**注：原创文章，转载请注明出处**
